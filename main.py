import time
import re
import os

import tiktoken
from loguru import logger
from openai import OpenAI
from vkbottle.bot import Bot, Message
from dotenv import load_dotenv
from vkbottle_types.objects import MessagesMessageAttachmentType, PhotosPhotoSizes

load_dotenv()

bot = Bot(os.environ["VK_API_KEY"])
client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])
cooldown = 0

SYSTEM_MSG = "You are an assistant in a group chat. Answer in user's language. Never include links in your answers (anything that's separated by a period without spaces, like fizz.buzz, my.site, etc.)"
SYSTEM_EMOJI = "‚öôÔ∏è"
AI_EMOJI = "ü§ñ"
MAX_WIDTH = 750
BAN_WORDS = ("hitler", "–≥–∏—Ç–ª–µ—Ä", "gitler", "–Ω–∏–≥–≥–µ—Ä", "–Ω–µ–≥—Ä", "vto.pe", "vtope")
AI_BAN_WORDS = ("—Å–∏–Ω–∏–π –∫–∏—Ç", "—Å–æ–≤–∞ –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ —Å–ø–∏—Ç",)
CENSOR_WORDS = ("onion", "hitler", "vtope", "vto.pe", "vto pe")


def num_tokens_from_string(string: str, model: str = "gpt-3.5-turbo") -> int:
    encoding = tiktoken.encoding_for_model(model)
    num_tokens = len(encoding.encode(string))
    return num_tokens


def create_response(question: str, img: str = None, model: str = "gpt-3.5-turbo") -> str:
    if img is not None:
        if model != "gpt-4-vision-preview":
            raise ValueError("Only \"gpt-4-vision-preview\" model can understand images")
        response = client.chat.completions.create(
            model=model,
            max_tokens=300,
            messages=[
                {"role": "system", "content": SYSTEM_MSG},
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": question},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": img,
                            },
                        },
                    ],
                }
            ],
        )
    else:
        response = client.chat.completions.create(
            model=model,
            max_tokens=1000,
            messages=[
                {"role": "system", "content": SYSTEM_MSG},
                {"role": "user", "content": question}
            ]
        )

    return f"{AI_EMOJI} {response.choices[0].message.content}"


def is_flagged(question: str) -> tuple:
    moderation = client.moderations.create(input=question)
    is_flagged = moderation.results[0].flagged
    moderation_dict = moderation.model_dump()
    categories_dict = moderation_dict['results'][0]['categories']

    if is_flagged:
        flagged = []
        for category in categories_dict:
            is_flagged = categories_dict[category]
            if is_flagged:
                flagged.append(category)

        return (True, ', '.join(flagged))
    return (False, '')


def pick_img(sizes: list[PhotosPhotoSizes]) -> str:
    sizes_widths = [photo.width for photo in sizes]
    filtered_sizes = [size for size in sizes_widths if size <= MAX_WIDTH]

    if not filtered_sizes:
        closest_size = min(sizes_widths, key=lambda x: abs(x - MAX_WIDTH))
    else:
        closest_size = max(filtered_sizes)

    for photo in sizes:
        if photo.width == closest_size:
            photo_url = photo.url
            break

    return photo_url


@bot.on.message(text="!aihelp")
async def ai_help(message: Message):
    return (
        f"{SYSTEM_EMOJI} !ai <–∑–∞–ø—Ä–æ—Å> - –∑–∞–ø—Ä–æ—Å –∫ –±–æ—Ç—É (gpt-3.5-turbo)"
        "\n\n–ü—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —ç—Ç–æ–π –∫–æ–º–∞–Ω–¥—ã, gpt –ø–æ–ª—É—á–∞–µ—Ç –≤–∞—à –∑–∞–ø—Ä–æ—Å –∏ –≤–∞—à–µ –∏–º—è –∏ —Ñ–∞–º–∏–ª–∏—é (—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ, —Ä–∞–∑—Ä–µ—à–∞—è –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å —ç—Ç–∏ –¥–∞–Ω–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏ OpenAI)."
        "\n\n–ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç—É –∫–æ–º–∞–Ω–¥—É, –æ—Ç–≤–µ—Ç–∏–≤ –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ –¥—Ä—É–≥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, —Ç–æ gpt –ø–æ–ª—É—á–∞–µ—Ç –≤–∞—à –∑–∞–ø—Ä–æ—Å, –≤–∞—à–µ –∏–º—è –∏ —Ñ–∞–º–∏–ª–∏—é, —Ç–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –æ—Ç–≤–µ—Ç–µ –∏ –∏–º—è –∏ —Ñ–∞–º–∏–ª–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –æ—Ç–≤–µ—Ç–µ."
        "\n\n–ü—Ä–∏–ª–æ–∂–∏–≤ –∫–∞—Ä—Ç–∏–Ω–∫—É –∫ —Å–æ–æ–±—â–µ–Ω–∏—é, –±–æ—Ç –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å gpt-4-vision-preview, —á—Ç–æ–±—ã —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å –µ—ë (—Ç–æ–ª—å–∫–æ –¥–ª—è –∏–∑–±—Ä–∞–Ω–Ω—ã—Ö)."
    )


@bot.on.message(text=('!ai <question_user>', '!gpt3 <question_user>'))
async def ai_txt(message: Message, question_user: str):
    global cooldown
    if cooldown + 8 > time.time():
        return f"{SYSTEM_EMOJI} –ö—É–ª–î–∞—É–Ω!"

    if len(question_user) < 5:
        return f"{SYSTEM_EMOJI} –í –∑–∞–ø—Ä–æ—Å–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –±–æ–ª—å—à–µ 5 –±—É–∫–≤!"

    img_url = None
    if (
        message.attachments and
        message.attachments[0].type is MessagesMessageAttachmentType.PHOTO
    ):
        if message.from_id != 322615766:
            return f"{SYSTEM_EMOJI} –ù–µ–∞!"
        img_url = pick_img(message.attachments[0].photo.sizes)
    elif (
        message.reply_message and
        message.reply_message.attachments and
        message.reply_message.attachments[0].type is MessagesMessageAttachmentType.PHOTO
    ):
        if message.from_id != 322615766:
            return f"{SYSTEM_EMOJI} –ù–µ–∞!"
        img_url = pick_img(message.reply_message.attachments[0].photo.sizes)

    try:
        user = await message.get_user()
        question = f"{user.first_name} {user.last_name} "
    except Exception as e:
        logger.error(f"Couldn't add user's name (group?): {e}")
        question = ""

    if message.reply_message is not None:
        try:
            reply_user = await bot.api.users.get(user_ids=message.reply_message.from_id)
            full_name = f' (Reply user full name: "{reply_user[0].first_name} {reply_user[0].last_name}")'
        except Exception as e:
            logger.error(f"Couldn't add reply user's name (group?): {e}")
            full_name = ""
        question += f'[User answered to this message{full_name}: "{message.reply_message.text}"] '

    question += ': ' + question_user
    num_tokens = num_tokens_from_string(question)
    if num_tokens > 500:
        return f"{SYSTEM_EMOJI} –í —Å–æ–æ–±—â–µ–Ω–∏–∏ –±–æ–ª–µ–µ 500 —Ç–æ–∫–µ–Ω–æ–≤ ({num_tokens})! –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ–Ω—å—à–µ —Å–ª–æ–≤."

    if any(ban_word in question.lower() for ban_word in BAN_WORDS):
        return f"{SYSTEM_EMOJI} –ü–æ–ø—Ä–æ–±—É–π –ø–æ–≥–æ–≤–æ—Ä–∏—Ç—å –æ —á–µ–º-—Ç–æ –¥—Ä—É–≥–æ–º. –ü–æ–º–æ–∂–µ—Ç –≤ —Ä–∞–∑–≤–∏—Ç–∏–∏."

    try:
        flagged = is_flagged(question)
    except Exception as e:
        return f"{SYSTEM_EMOJI} –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –º–æ–¥–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞: {e}"

    if flagged[0] is True:
        return (
            f"{SYSTEM_EMOJI} –õ–∏–ª –±—Ä–æ –ø–æ–ø—ã—Ç–∞–ª—Å—è –∑–∞–±–∞–Ω–∏—Ç—å –º–µ–Ω—è, –Ω–æ —É –Ω–µ–≥–æ –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å :(\n"
            f"–ó–∞–ø—Ä–æ—Å –Ω–∞—Ä—É—à–∞–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ OpenAI: {flagged[1]}"
        )

    cooldown = time.time()
    try:
        if img_url is not None:
            ai_response = create_response(question, img_url, "gpt-4-vision-preview")
        else:
            ai_response = create_response(question)
    except Exception as e:
        return f"{SYSTEM_EMOJI} –ß–µ—Ç –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫: {e}"

    logger.info(ai_response)

    if any(ban_word in ai_response.lower() for ban_word in AI_BAN_WORDS) or re.search(r"[a-zA-Z–∞-—è–ê-–Ø]\.[a-zA-Z–∞-—è–ê-–Ø]", ai_response):
        return f"{SYSTEM_EMOJI} –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –æ–∫–∞–∑–∞–ª–æ—Å—å —Å–ª–æ–≤–æ –∏–∑ —á–µ—Ä–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞. –°–ø–∞—Å–∏–±–æ, —á—Ç–æ –ø–æ—Ç—Ä–∞—Ç–∏–ª –º–æ–∏ 0.0020 —Ü–µ–Ω—Ç–æ–≤."

    for censor in CENSOR_WORDS:
        ai_response = ai_response.replace(censor, "***")

    return ai_response


if __name__ == "__main__":
    logger.info("Starting bot...")
    bot.run_forever()
